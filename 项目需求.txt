第一部分：仿真环境设计（env. py）
1. 1 环境基本设置
地图大小：80×80 网格
局部观测范围：40×40 鸟瞰图（BEV）
地图类型：支持4种地图（simple、complex、concave、narrow）
1. 2 观测空间设计
环境在每个时间步提供以下观测信息：
图像观测（3通道×40×40）：
通道0：静态障碍物（墙壁、固定障碍）
通道1：全局引导路径（A*规划的路径点）
通道2：动态障碍物（移动的物体）
向量观测（2维）：
机器人当前线速度
机器人当前角速度
1.3 动作空间设计
动作维度：2维连续动作
动作范围：[-1, 1]
动作含义：
  - action[0]：线速度命令（映射后范围[0, 1]）
  - action[1]：角速度命令（范围[-1, 1]）
1.4 动态障碍物建模
数量：简单地图3个，复杂地图6个
运动模式：随机初始速度，边界反弹
更新方式：每个时间步根据速度更新位置
1.5 奖励函数设计
进展奖励（r_progress）：鼓励靠近目标点，系数30. 0
危险惩罚（r_danger）：接近动态障碍物时惩罚，距离<3. 0时触发
时间惩罚（r_step）：每步-0.05，鼓励快速到达
碰撞惩罚（r_collision）：碰撞时-50. 0，回合终止
成功奖励（r_success）：到达目标时+100.0，回合终止
1.6 机器人运动学模型
速度平滑：v_new = 0.7 * v_old + 0.3 * v_cmd
方向更新：agent_dir += omega * 0.6
位置更新：
  - x += v * cos(agent_dir)
  - y += v * sin(agent_dir)
第二部分：全局路径规划器（global_planner.py）
2.1 算法选择
使用A*算法进行全局路径规划
输入：
  - 静态地图（80×80网格，1表示障碍，0表示可通行）
  - 起点坐标
  - 终点坐标
输出：
  - 路径点序列（从起点到终点的坐标列表）
特性：
  - 支持8邻域搜索（包括对角线方向）
  - 启发式函数：欧几里得距离
  - 最大搜索步数限制：5000步（防止死循环）
  - 兜底处理：如果起点或终点在障碍物内，自动寻找最近的可通行点
第三部分：神经网络模型（model.py）
输入 → CNN特征提取 → 多头自注意力 → 多头交叉注意力 → 特征压缩 → 特征融合 → GRU时序建模 → Actor/Critic输出
3.2 CNN特征提取模块
输入：3通道×40×40图像
网络结构：
  - Conv2d(3, 32, kernel_size=8, stride=4) + ReLU
  - Conv2d(32, 64, kernel_size=4, stride=2) + ReLU
  - Conv2d(64, 64, kernel_size=3, stride=1) + ReLU
输出：特征图（保留空间维度，用于后续注意力处理）
3.3 多头自注意力模块（Self-Attention）
目的：评估局部区域内的综合风险，动态关注重要的障碍物区域

输入：CNN输出的特征图，展平为特征序列 [Batch, N, C]
      其中 N = H × W（空间位置数量），C = 通道数

处理：
  - 将特征序列作为Query、Key、Value输入多头自注意力
  - 计算任意两个空间位置之间的相关性
  - 动态分配注意力权重（危险区域权重高，无关区域权重低）

输出：经过风险加权的特征序列 [Batch, N, C]

参数：
  - embed_dim：特征维度（与CNN输出通道数一致）
  - num_heads：注意力头数（建议4或8）
3. 4 多头交叉注意力模块（Cross-Attention）
目的：结合全局路径引导，使模型关注与路径相关的区域

输入：
  - Query：全局路径特征（从通道1提取或单独编码）
  - Key/Value：自注意力输出的环境特征序列

处理：
  - 计算路径特征与环境特征之间的相关性
  - 突出与当前路径方向相关的区域

输出：路径引导下的聚焦特征序列 [Batch, N, C]

参数：
  - embed_dim：特征维度
  - num_heads：注意力头数（建议4或8）
3.5 特征压缩与融合模块
特征压缩：
  - 将注意力输出的特征序列 [Batch, N, C] 压缩为固定向量 [Batch, C]
  - 方式：全局平均池化（Global Average Pooling）

速度向量编码：
  - 输入：2维速度向量
  - 网络：Linear(2, 32) + ReLU
  - 输出：32维速度特征

特征融合：
  - 将压缩后的图像特征与速度特征拼接
  - 输出：融合特征向量 [Batch, C + 32]
3.6 GRU时序建模模块
目的：记住动态障碍物的运动趋势，捕捉时序依赖关系

输入：
  - 当前时刻的融合特征向量
  - 上一时刻的隐藏状态 [Batch, hidden_dim]

网络：GRUCell(input_dim=融合特征维度, hidden_dim=256)

输出：更新后的隐藏状态 [Batch, 256]
3.7 Actor-Critic输出模块
Actor网络（策略网络）：
  - 输入：GRU输出的状态向量 [Batch, 256]
  - 网络：Linear(256, 128) + ReLU + Linear(128, 2) + Tanh
  - 输出：动作均值 mu [Batch, 2]，范围[-1, 1]
  - 标准差：可学习参数 log_std，通过exp转换为std

Critic网络（价值网络）：
  - 输入：GRU输出的状态向量 [Batch, 256]
  - 网络：Linear(256, 128) + ReLU + Linear(128, 1)
  - 输出：状态价值估计 V [Batch, 1]
第四部分：训练算法（train. py）
4.1 算法选择
使用PPO（Proximal Policy Optimization）算法进行策略优化。
4.2 超参数设置
学习率：3e-4
学习率衰减：每200步衰减，衰减系数0.95
折扣因子（gamma）：0.99
PPO裁剪系数（eps_clip）：0. 2
每次更新的epoch数：10
批次大小：64
更新间隔：每1000步更新一次
最大训练回合数：10000
熵系数：0.02
梯度裁剪：max_norm=0.5
4.3 经验存储
存储内容：
  - 图像观测序列
  - 向量观测序列
  - 动作序列
  - 动作对数概率序列
  - 奖励序列
  - 终止标志序列
  - GRU隐藏状态序列
4. 4 训练流程
1. 初始化环境、规划器、PPO智能体
2.  对于每个训练回合：
   a. 重置环境，获取初始观测
   b. 使用A*规划器生成全局路径
   c. 初始化GRU隐藏状态为零向量
   d.  对于每个时间步：
      - 根据当前观测和隐藏状态，选择动作
      - 执行动作，获取下一观测、奖励、终止标志
      - 存储经验到Memory
      - 如果达到更新间隔，执行PPO更新
      - 如果回合终止，跳出循环
   e. 定期评估策略性能并保存模型
4.5 课程学习策略
训练初期：主要在简单地图（simple）上训练
训练后期：逐渐增加复杂地图（complex、narrow）的比例
切换概率：prob_complex = min(1.0, episode / 5000)
4.6 评估方式
评估地图：固定使用complex地图
评估回合数：每次评估5个回合
评估指标：平均累计奖励
评估频率：每50个训练回合评估一次
第五部分：地图生成器（map_generator.py）
5.1 地图类型
simple：简单静态环境，约25个分散的4×4小方块障碍物
complex：复杂静态环境，23个4×4小方块 + 7个6×6大方块
concave：凹型障碍物环境，包含L型、T型、凹型障碍物组合
narrow：狭窄通道环境，包含大型障碍物和狭窄通行区域
5.2 地图生成要求
地图大小：80×80
边界：所有地图四周有2格宽的边界墙
起点：默认(5, 5)
终点：默认(70, 70)或(75, 75)
输出：
  - grid：numpy数组，1表示障碍，0表示可通行
  - start_pos：起点坐标
  - goal_pos：终点坐标
第六部分：文件结构
project/
├── env.py              # 仿真环境
├── global_planner.py   # A*全局规划器
├── map_generator.py    # 地图生成器
├── model.py            # 神经网络模型（含双重注意力机制）
├── train.py            # PPO训练脚本
└── models/             # 模型保存目录
第七部分：关键实现要点
7.1 双重注意力机制的连接方式
CNN输出特征图 [B, C, H, W]
    ↓
展平为序列 [B, N, C]，其中 N = H × W
    ↓
多头自注意力：Q=K=V=特征序列 → 输出风险加权特征 [B, N, C]
    ↓
多头交叉注意力：Q=路径特征，K=V=自注意力输出 → 输出路径引导特征 [B, N, C]
    ↓
全局平均池化压缩为向量 [B, C]
    ↓
与速度特征拼接 [B, C+32]
    ↓
输入GRU进行时序建模
7. 2 路径特征的提取方式
方式一：从CNN输入的通道1（全局路径通道）单独提取
方式二：使用单独的小型CNN编码路径通道
方式三：将路径点坐标序列通过Embedding编码
7.3 注意力模块的位置编码
由于注意力机制本身不包含位置信息，需要添加位置编码：
- 可使用2D正弦位置编码
- 或使用可学习的位置嵌入
